# ğŸŒ¿ BranchyNet Replication â€“ Fast Inference DNN

This repository provides a **PyTorch-based replication** of the  
**BranchyNet: Fast Inference via Early Exiting from Deep Neural Networks**.

The focus is **understanding how early-exit branches reduce inference time**  
rather than achieving state-of-the-art accuracy.

- Backbone DNN with **side branches** ğŸ  
- Early exits allow confident samples to leave network ğŸ¬  
- Entropy-based **exit decision** ğŸƒ  
- Joint optimization improves **accuracy & gradient flow** ğŸŒ±  

**Paper reference:** [BranchyNet â€“ Teerapittayanon et al., 2017](https://arxiv.org/abs/1709.01686) ğŸŒ´

---

## ğŸŒŒ Overview â€“ BranchyNet Architecture

![BranchyNet Example](images/figmix.jpg)

## ğŸš€ High-level Pipeline

1. **Input image**

```math
x \in \mathbb{R}^{C \times H \times W}, \quad \text{feat} = f_{\text{backbone}}(x)
```

2. **Branch outputs**

```math
z_n = f_{\text{branch}_n}(\text{feat}), \quad
\hat{y}_n = \text{softmax}(W_n z_n + b_n)
```

3. **Entropy at each exit**

```math
H(\hat{y}_n) = - \sum_{c \in \mathcal{C}} \hat{y}_{n,c} \log(\hat{y}_{n,c})
```

4. **Early exit decision**

```math
\text{if } H(\hat{y}_n) < T_n, \text{ exit with } \hat{y}_n
```

5. **Final classifier**

```math
z_f = \text{feat} \quad (\text{or last branch output}), \quad
\hat{y}_{\text{final}} = \text{softmax}(W_f z_f + b_f)
```



---

## ğŸ§  What the Model Learns

- **Backbone**: hierarchical feature extraction  
- **Branches**: allow confident samples to exit early â†’ faster inference  
- **Joint loss**:
```math
\mathcal{L}_{\text{BranchyNet}} = \sum_{n=1}^{N} w_n \cdot \mathcal{L}_n(\hat{y}_n, y)
```

   regularizes both main and side branches  
- **Entropy threshold**: balances speed vs. accuracy

---

## ğŸ“¦ Repository Structure

```bash
BranchyNet-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_block.py            # Reusable Conv + activation block for feature extraction
â”‚   â”‚   â”œâ”€â”€ activation.py            # Activation functions (ReLU, LeakyReLU, Sigmoid, etc.)
â”‚   â”‚   â”œâ”€â”€ normalization.py         # Normalization layers (BatchNorm, LayerNorm)
â”‚   â”‚   â””â”€â”€ pooling.py               # Pooling operations (MaxPool, AvgPool)
â”‚   â”‚
â”‚   â”œâ”€â”€ exits/
â”‚   â”‚   â”œâ”€â”€ entropy.py               # Entropy computation for early-exit confidence
â”‚   â”‚   â””â”€â”€ exit_decision.py         # Threshold-based early-exit decision logic
â”‚   â”‚
â”‚   â”œâ”€â”€ blocks/
â”‚   â”‚   â”œâ”€â”€ backbone_block.py        # Backbone network blocks (VGG/ResNet-style)
â”‚   â”‚   â”œâ”€â”€ branch_block.py          # Side branch attached to the backbone
â”‚   â”‚   â””â”€â”€ classifier_head.py       # Lightweight classifier for each exit
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ branchynet.py            # Full model: backbone with multiple early exits with Forward pass with early-exit control flow
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ joint_loss.py            # Weighted sum of losses from all exits (theoretical)
â”‚   â”‚
â”‚   â””â”€â”€ config.py                    # Number of exits, entropy thresholds, loss weights
â”‚
â”œâ”€â”€ diagrams/
â”‚   â””â”€â”€ figmix.jpg                 
â”‚
â”œâ”€â”€ requirements.txt                
â””â”€â”€ README.md                     

```
---


## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
